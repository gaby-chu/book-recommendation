{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project part III: Research report\n",
    "\n",
    "We've put some effort into building our collection of volumes - you can find them in [this Google Drive folder](https://drive.google.com/drive/u/0/folders/1UAaGIiqElF9YLTGIy6hQYM7QcGzosZgR). Now it's time to learn something about it. You already have lots of excellent ideas for how to apply the tools we've learned about so far. It's also a good time in the semester to review what we have learned and practice applying it in less structured settings.\n",
    "\n",
    "**You will work by yourself or in a group of up to three people** to complete a short project applying methods from the previous weeks to this collection. You will turn in the completed project as a single notebook (one submission per group) with the following sections:\n",
    "\n",
    "0. **Project team.** List members with full names and NetIDs. If your group does not contain at least one native speaker of English, let us know.\n",
    "\n",
    "1. **Question(s) (10 points).** Describe what you wanted to learn. Suggest several possible answers or hypotheses, and describe in general terms what you might expect to see if each of these answers were true (save specific measurements for the next section). For example, many students want to know the difference between horror and non-horror fiction, or between detective stories and horror fiction, but there are many ways to operationalize this question. You do not need to limit yourself to questions of genre. **Note that your question should be interesting! If the answer is obvious before you begin, or if it's something the importance of which you cannot explain, your grade will suffer (a lot).** \n",
    "\n",
    "1. **Methods (10 points).** Describe how you will use computational methods presented so far in this class to answer your question. What do the computational tools do, and how does their output relate to your question? Describe how you will process the collection into a form suitable for a model or algorithm and why you have processed it the way you have.\n",
    "\n",
    "1. **Code (20 points).** Carry out your experiments. Code should be correct (no errors) and focused (unneeded code from examples is removed). Use the notebook format effectively: code may be incorporated into multiple sections.\n",
    "\n",
    "1. **Results and discussion (40 points).** Use sorted lists, tables, and visual presentations to make your argument. Excellent projects will provide multiple views of results, and follow up on any apparent outliers or strange cases, including through careful reading of the original documents.\n",
    "\n",
    "1. **Reflection (10 points).** Describe your experience in this process. What was harder or easier than you expected? What compromises or negotiations did you have to accept to match the collection, the question, and the methods? What would you try next? Your reflections should be written as single narrative that incorporates the viewpoints of all group members.\n",
    "\n",
    "1. **Resources consulted (0 points, but -5 if missing).** Credit any online sources (Stack Overflow, blog posts, documentation, etc.) that you found helpful.\n",
    "\n",
    "1. **Responsibility statement (0 points, but -5 if missing).** See separate CMS assignment \"MP 03: Responsibility statement\". **Note:** If you worked alone on the project (a group of one), you are not required to submit the responsibility statement.\n",
    "\n",
    "\n",
    "## Submission instructions\n",
    "\n",
    "1. Complete this Jupyter Notebook.\n",
    "1. Open a group submission on CMS - Let us know if you encounter any problems with that!\n",
    "1. Assign the role of group submitter to a member of your group.\n",
    "1. Submit the completed Jupyter Notebook in the group submission on CMS.\n",
    "\n",
    "**Note:** If you worked alone on the project (a group of one), you are not required to submit the responsibility statement.\n",
    "\n",
    "Otherwise...\n",
    "\n",
    "1. Go to the CMS assignment 'MP 03: Responsibility statement'.\n",
    "1. Complete the statement, describing each group member's contributions as you see them. \n",
    "1. Upload the statement as an **individual** submission on CMS.\n",
    "\n",
    "\n",
    "## Guidance and advice\n",
    "\n",
    "Show us what you've learned so far. Try to use a range of methods while remaining focused on your chosen problem. For inspiration, consider the range of research problems you've enoucountered in the readings. Note that dictionary-based sentiment scoring projects have not historically done well without substantial additional methodological diversity.\n",
    "\n",
    "**We will grade this work based on accuracy, thoroughness, creativity, reflectiveness, and quality of presentation.** Code and results that are merely correct are generally not enough, on their own, to achieve a high score.\n",
    "\n",
    "**Scope:** this is a *mini*-project, with a short deadline. We are expecting work that is consistent with that timeframe, but that is serious, thoughtful, and rigorous. This assignment will almost certainly require more time and effort than the typical weekly homework. **For group work, the expected scope grows linearly with the number of participants.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Project team\n",
    "\n",
    "Estelle Hooper (ehh52), Gabriella Chu (gc386)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Question(s) (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Research Question: How can we recommend books of other genres to lovers of a single genre?\n",
    "#### Motivation \n",
    "When choosing to consume any form of entertainment, such as a TV show, play, or movie, people tend to favor a particular genre or gravitate towards a favorite story. Unlike the aforementioned examples, people tend to consume books or novels at their own pace, which could be considered more time consuming and costly in terms of mental effort to stay committed to a written story. Because time and attention are scarce, people are more likely to read books of one genre. Many people do not have the capacity to branch out and try new genres, feeling that the safer choice is to read a more predictable book of their favored genre rather than risk not enjoying content of a different genre.\n",
    "#### Hypothesis\n",
    "In order to have confidence in another genre, there must be overlapping qualities between a reader's favorite genre and the new genre. Fans of the horror genre may be fond of the setting because horror fiction can often involve supernatural beings or powers, such as ghosts. Therefore, they may also enjoy science fiction or fantasy that similarly incorporates elements outside of the natural world. Besides content, genres may overlap in how the genre's authors tend to portray characters. Adventure and detective genres tend to involve a partner-in-crime in which the two characters develop strong trust. If the character interactions are important, then perhaps fans of the detective genre would also be a fan of the romance genre since the latter involves the development of a relationship between characters.\n",
    "#### Project\n",
    "Our corpus consists of a good sample of horror novels, detective novels, and a miscellanous group of novels from other genres. We want to see how close these \"other novels\" are to either the horror genre or detective genre. We will train a classifier using the detective and horror novels, and run that classifier on the \"other\" novels. Those \"other\" novels will receive a horror or detective label (despite not being canonically considered part of the horror or detective genres). With these classifier outcomes, we will recommend the \"other\" books labeled \"detective\" to fans of the detective genre and those labeled \"horror\" to the horror genre. For example, the classifier ran on *Harry Potter*, a fantasy series, and gave it a detective label, we would recommend detective fans to read *Harry Potter.*\n",
    "#### Expectations\n",
    "Overall, we don't expect the recommendations we obtain from the classifer to be very great because it is only working with detective and horror, and it will be hard to find overlapping words with only those two genres and the various other genres in the corpus. We expect detective and horror novels with the most subplots and settings similar to other genres to score well with those genres. For example, if detective novels tend to have romance subplots, we expect the classifier to label romance novels as detective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Preparation\n",
    "From the class corpus, we will divide the novels into two subsets: horror and detective, and the rest of the novels. The horror and detective will act as our \"training\" dataset because we will train the classifier to classify text as either **detective (==1) or horror (==0).** All the other novels of other genres will act as our \"testing\" dataset, and the classifier will predict these novels as either horror or detective. We will use the detective column in the metadata to get the gold labels of the training data in order to score our model later on. We'll also read/open all the novels in this step.\n",
    "### II. Vectorization\n",
    "We will vectorize the novels with tfidf weighting, L2, and z-scores to standardize and normalize our corpus, and consider pre-processing factors like stopwords and lowercasing. Them, we'll create multilple vectorizers with a variety of max input features and fit the training data (true horror and detective). We will try different combinations of these matrices (each with a different number input features) with different types of classifiers in the next step.\n",
    "### III. Classifier\n",
    "We will test different classification methods with different parameters (Multinomial Naive Bayes and Logistic Regression) with our X_train matrix of different sizes and the gold labels (y_train) and compare the accuracy/precision/recall/F1 scores and select the best classifier.\n",
    "### IV. Feature Importance \n",
    "After finding the model with the best F1 score, we will examine the words/tokens/features with the heaviest weights that aid in classifying a work as detective or horror. If we see odd words, such as character names, or common stopwords as the top features, we will reconsider our vectorization steps.\n",
    "### V. Testing/Predicting\n",
    "After finalizing our classifier, we will run it on our testing data, or the novels of other genres without detective or horror labels. From this classifier, each \"other\" novel will receive a horror or detective label (y_test).\n",
    "### VI. Scatterplots/Visualizations (to be discussed in Part 4: Results)\n",
    "Because the classifier only makes binary decisions, we want to examine the most \"detective-y\" and \"horror-y\" novels because there will be some novels that the classifer could not make as definitive decisions for. We will use SVD to graph all the novels in 2D space. We will then create multiple scatterplots to look at these decision boundaries. The most important scatterplots will be the ones that look at how close the true labeled novels are to the predicted  novels are in distance space. We will consider the predicted dots that are closest to the true dots to be the best recommendations for that genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (all of them!)\n",
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from   sklearn.decomposition import TruncatedSVD\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from   sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from   sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preparation\n",
    "In this section, we cleaned and manipulated the class metadata in order to subset the data into training and testing. As stated previously, our training data is all horror or detective, and our testing is all other books that are not horror and detective. The following is a list of variables that we created to accomplish in order to proceed with vectorization. Besides subsetting, the most notable tasks we did were sorting the dataframes by title in alphabetical order and obtaining the gold labels (y_train), which will be used to cross_validate our model when we build our classifer. \n",
    "- `metadata` df; metadata spreadsheet from class corpus\n",
    "- `training_data` df; metadata with training novels, received by subsetting metadata Detective==True | Horror==True\n",
    "- `testing_data` df; metadata with testing novels, received by subsetting metadata Detective==True & Horror==True\n",
    "- `training_names` list; list of filenames, recieved by training_data.filename.values\n",
    "- `testing_names` list; list of filenames, recieved by testing_data.filename.values\n",
    "- `training_books` list; list of strings/books, recieved by reading files using training_names\n",
    "- `testing_books` list; list of strings/books, recieved by reading files using testing_names\n",
    "- `y_train` list; the **gold labels (1=detective, 0=horror)**, or true values of the training data, obtained from the training_data detective column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 34)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class corpus metadata\n",
    "metadata = pd.read_csv(\"class_corpus_metadata.csv\")\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>check_1</th>\n",
       "      <th>check_2</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>author1_surname</th>\n",
       "      <th>author1_givenname</th>\n",
       "      <th>author2_surname</th>\n",
       "      <th>author2_givenname</th>\n",
       "      <th>gender_author1</th>\n",
       "      <th>...</th>\n",
       "      <th>feminist fiction</th>\n",
       "      <th>mystery</th>\n",
       "      <th>adventure</th>\n",
       "      <th>tragedy</th>\n",
       "      <th>children</th>\n",
       "      <th>regency</th>\n",
       "      <th>manners</th>\n",
       "      <th>philosophical</th>\n",
       "      <th>coming-of-age</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsg57</td>\n",
       "      <td>scw222</td>\n",
       "      <td>lcc82</td>\n",
       "      <td>Writings in the United Amateur, 1915 - 1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>Lovecraft</td>\n",
       "      <td>Howard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Lovecraft_WritingsintheUnitedAmateur1915-1922.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fhh26</td>\n",
       "      <td>gs542</td>\n",
       "      <td>tj256</td>\n",
       "      <td>Whose Body?</td>\n",
       "      <td>1923</td>\n",
       "      <td>Sayers</td>\n",
       "      <td>Dorothy L.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Sayres_WhoseBody.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cl2264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Voodoo Planet</td>\n",
       "      <td>1959</td>\n",
       "      <td>Norton</td>\n",
       "      <td>Andre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Norton_VoodooPlanet.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ehh52</td>\n",
       "      <td>sjr255</td>\n",
       "      <td>kg428</td>\n",
       "      <td>Varney the Vampire; Or, the Feast of Blood by ...</td>\n",
       "      <td>1845</td>\n",
       "      <td>Rymer</td>\n",
       "      <td>James Malcolm</td>\n",
       "      <td>Prest</td>\n",
       "      <td>Thomas Peckett</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Prest_Rhymer_VarneyTheVampire.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dgr73</td>\n",
       "      <td>jlp367</td>\n",
       "      <td>kg428</td>\n",
       "      <td>Uncle Tom's Cabin</td>\n",
       "      <td>1852</td>\n",
       "      <td>Stowe</td>\n",
       "      <td>Harriet Beecher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Stowe_UncleTom_sCabin.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 check_1 check_2  \\\n",
       "0      nsg57  scw222   lcc82   \n",
       "1      fhh26   gs542   tj256   \n",
       "2     cl2264     NaN     NaN   \n",
       "3      ehh52  sjr255   kg428   \n",
       "4      dgr73  jlp367   kg428   \n",
       "\n",
       "                                               title  year author1_surname  \\\n",
       "0        Writings in the United Amateur, 1915 - 1922  1922       Lovecraft   \n",
       "1                                        Whose Body?  1923          Sayers   \n",
       "2                                      Voodoo Planet  1959          Norton   \n",
       "3  Varney the Vampire; Or, the Feast of Blood by ...  1845           Rymer   \n",
       "4                                  Uncle Tom's Cabin  1852           Stowe   \n",
       "\n",
       "  author1_givenname author2_surname author2_givenname gender_author1  ...  \\\n",
       "0            Howard             NaN               NaN           Male  ...   \n",
       "1        Dorothy L.             NaN               NaN         Female  ...   \n",
       "2             Andre             NaN               NaN         Female  ...   \n",
       "3     James Malcolm           Prest    Thomas Peckett           Male  ...   \n",
       "4   Harriet Beecher             NaN               NaN         Female  ...   \n",
       "\n",
       "  feminist fiction mystery adventure tragedy children  regency  manners  \\\n",
       "0            False    True     False   False    False    False    False   \n",
       "1            False    True     False   False    False    False    False   \n",
       "2            False   False      True   False    False    False    False   \n",
       "3            False   False     False   False    False    False    False   \n",
       "4            False   False     False   False    False    False    False   \n",
       "\n",
       "  philosophical coming-of-age  \\\n",
       "0          True         False   \n",
       "1         False         False   \n",
       "2         False         False   \n",
       "3         False         False   \n",
       "4         False         False   \n",
       "\n",
       "                                            filename  \n",
       "0  Lovecraft_WritingsintheUnitedAmateur1915-1922.txt  \n",
       "1                               Sayres_WhoseBody.txt  \n",
       "2                            Norton_VoodooPlanet.txt  \n",
       "3                  Prest_Rhymer_VarneyTheVampire.txt  \n",
       "4                          Stowe_UncleTom_sCabin.txt  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data are books that are either horror or detective\n",
    "training_data = metadata[(metadata['horror']==True) | (metadata['detective']==True)]\n",
    "\n",
    "# drop books that are both horror and detective\n",
    "drop = metadata[(metadata['horror']==True) & (metadata['detective']==True)]\n",
    "training_data = training_data.drop(drop.index)\n",
    "\n",
    "# testing data are books are neither horror or detective\n",
    "testing_data = metadata[(metadata['horror']==False) & (metadata['detective']==False)]\n",
    "\n",
    "# sort titles alphabetically \n",
    "training_data = training_data.sort_values('title')\n",
    "testing_data = testing_data.sort_values('title')\n",
    "# note: training+testing+dropped row = 159 rows, class corpus = 160 rows, \"An Unkindness of Ghosts\" has no input for horror and detective column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 80 combined horror and detective novels in the corpus that we will use to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>check_1</th>\n",
       "      <th>check_2</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>author1_surname</th>\n",
       "      <th>author1_givenname</th>\n",
       "      <th>author2_surname</th>\n",
       "      <th>author2_givenname</th>\n",
       "      <th>gender_author1</th>\n",
       "      <th>...</th>\n",
       "      <th>feminist fiction</th>\n",
       "      <th>mystery</th>\n",
       "      <th>adventure</th>\n",
       "      <th>tragedy</th>\n",
       "      <th>children</th>\n",
       "      <th>regency</th>\n",
       "      <th>manners</th>\n",
       "      <th>philosophical</th>\n",
       "      <th>coming-of-age</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tl566</td>\n",
       "      <td>hz542</td>\n",
       "      <td>ja532</td>\n",
       "      <td>813</td>\n",
       "      <td>1910</td>\n",
       "      <td>Leblanc</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Leblanc_813.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gc386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Strange Disappearance</td>\n",
       "      <td>1998</td>\n",
       "      <td>Green</td>\n",
       "      <td>Anna Katharine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GreenAnnaKatharine_AStrangeDisappearance.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nca28</td>\n",
       "      <td>tl566</td>\n",
       "      <td>stw43</td>\n",
       "      <td>A Study in Scarlet</td>\n",
       "      <td>1887</td>\n",
       "      <td>Conan Doyle</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ConanDoyle_AStudyInScarlet.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jc2739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agatha Webb</td>\n",
       "      <td>1899</td>\n",
       "      <td>Green</td>\n",
       "      <td>Anna Katharine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Green_AgathaWebb.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lcc82</td>\n",
       "      <td>yk499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>1872</td>\n",
       "      <td>Le_Fanu</td>\n",
       "      <td>Joseph Sheridan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Carmilla.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tr333</td>\n",
       "      <td>sjs457</td>\n",
       "      <td>sl2324</td>\n",
       "      <td>The Valley of Fear</td>\n",
       "      <td>1915</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>Arthur Conan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Doyle_TheValleyOfFear.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>lrs263</td>\n",
       "      <td>sh785</td>\n",
       "      <td>hz542</td>\n",
       "      <td>The Wisdom of Father Brown</td>\n",
       "      <td>1914</td>\n",
       "      <td>Chesterton</td>\n",
       "      <td>Gilbert Keith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Chesterton_TheWisdomOfFatherBrown.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ehh52</td>\n",
       "      <td>sjr255</td>\n",
       "      <td>kg428</td>\n",
       "      <td>Varney the Vampire; Or, the Feast of Blood by ...</td>\n",
       "      <td>1845</td>\n",
       "      <td>Rymer</td>\n",
       "      <td>James Malcolm</td>\n",
       "      <td>Prest</td>\n",
       "      <td>Thomas Peckett</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Prest_Rhymer_VarneyTheVampire.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>fhh26</td>\n",
       "      <td>gs542</td>\n",
       "      <td>tj256</td>\n",
       "      <td>Whose Body?</td>\n",
       "      <td>1923</td>\n",
       "      <td>Sayers</td>\n",
       "      <td>Dorothy L.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Sayres_WhoseBody.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>nsg57</td>\n",
       "      <td>scw222</td>\n",
       "      <td>lcc82</td>\n",
       "      <td>Writings in the United Amateur, 1915 - 1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>Lovecraft</td>\n",
       "      <td>Howard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Lovecraft_WritingsintheUnitedAmateur1915-1922.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 check_1 check_2  \\\n",
       "0       tl566   hz542   ja532   \n",
       "1       gc386     NaN     NaN   \n",
       "2       nca28   tl566   stw43   \n",
       "3      jc2739     NaN     NaN   \n",
       "4       lcc82   yk499     NaN   \n",
       "..        ...     ...     ...   \n",
       "75      tr333  sjs457  sl2324   \n",
       "76     lrs263   sh785   hz542   \n",
       "77      ehh52  sjr255   kg428   \n",
       "78      fhh26   gs542   tj256   \n",
       "79      nsg57  scw222   lcc82   \n",
       "\n",
       "                                                title  year author1_surname  \\\n",
       "0                                                 813  1910         Leblanc   \n",
       "1                             A Strange Disappearance  1998           Green   \n",
       "2                                  A Study in Scarlet  1887     Conan Doyle   \n",
       "3                                         Agatha Webb  1899           Green   \n",
       "4                                            Carmilla  1872         Le_Fanu   \n",
       "..                                                ...   ...             ...   \n",
       "75                                 The Valley of Fear  1915           Doyle   \n",
       "76                         The Wisdom of Father Brown  1914      Chesterton   \n",
       "77  Varney the Vampire; Or, the Feast of Blood by ...  1845           Rymer   \n",
       "78                                        Whose Body?  1923          Sayers   \n",
       "79        Writings in the United Amateur, 1915 - 1922  1922       Lovecraft   \n",
       "\n",
       "   author1_givenname author2_surname author2_givenname gender_author1  ...  \\\n",
       "0            Maurice             NaN               NaN           Male  ...   \n",
       "1     Anna Katharine             NaN               NaN         Female  ...   \n",
       "2             Arthur             NaN               NaN           Male  ...   \n",
       "3     Anna Katharine             NaN               NaN         Female  ...   \n",
       "4    Joseph Sheridan             NaN               NaN           Male  ...   \n",
       "..               ...             ...               ...            ...  ...   \n",
       "75      Arthur Conan             NaN               NaN           Male  ...   \n",
       "76     Gilbert Keith             NaN               NaN           Male  ...   \n",
       "77     James Malcolm           Prest    Thomas Peckett           Male  ...   \n",
       "78        Dorothy L.             NaN               NaN         Female  ...   \n",
       "79            Howard             NaN               NaN           Male  ...   \n",
       "\n",
       "   feminist fiction mystery adventure tragedy children  regency  manners  \\\n",
       "0             False    True     False   False    False    False    False   \n",
       "1             False    True     False   False    False    False    False   \n",
       "2             False    True     False   False    False    False    False   \n",
       "3             False    True     False   False    False    False    False   \n",
       "4             False   False     False   False    False    False    False   \n",
       "..              ...     ...       ...     ...      ...      ...      ...   \n",
       "75            False   False     False   False    False    False    False   \n",
       "76            False   False     False   False    False    False    False   \n",
       "77            False   False     False   False    False    False    False   \n",
       "78            False    True     False   False    False    False    False   \n",
       "79            False    True     False   False    False    False    False   \n",
       "\n",
       "   philosophical coming-of-age  \\\n",
       "0          False         False   \n",
       "1          False         False   \n",
       "2          False         False   \n",
       "3          False         False   \n",
       "4          False         False   \n",
       "..           ...           ...   \n",
       "75         False         False   \n",
       "76         False         False   \n",
       "77         False         False   \n",
       "78         False         False   \n",
       "79          True         False   \n",
       "\n",
       "                                             filename  \n",
       "0                                     Leblanc_813.txt  \n",
       "1        GreenAnnaKatharine_AStrangeDisappearance.txt  \n",
       "2                      ConanDoyle_AStudyInScarlet.txt  \n",
       "3                                Green_AgathaWebb.txt  \n",
       "4                                        Carmilla.txt  \n",
       "..                                                ...  \n",
       "75                          Doyle_TheValleyOfFear.txt  \n",
       "76              Chesterton_TheWisdomOfFatherBrown.txt  \n",
       "77                  Prest_Rhymer_VarneyTheVampire.txt  \n",
       "78                               Sayres_WhoseBody.txt  \n",
       "79  Lovecraft_WritingsintheUnitedAmateur1915-1922.txt  \n",
       "\n",
       "[80 rows x 34 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=training_data.reset_index(drop=True)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 78 combined novels from a variety of genres that are not horror or detective in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>check_1</th>\n",
       "      <th>check_2</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>author1_surname</th>\n",
       "      <th>author1_givenname</th>\n",
       "      <th>author2_surname</th>\n",
       "      <th>author2_givenname</th>\n",
       "      <th>gender_author1</th>\n",
       "      <th>...</th>\n",
       "      <th>feminist fiction</th>\n",
       "      <th>mystery</th>\n",
       "      <th>adventure</th>\n",
       "      <th>tragedy</th>\n",
       "      <th>children</th>\n",
       "      <th>regency</th>\n",
       "      <th>manners</th>\n",
       "      <th>philosophical</th>\n",
       "      <th>coming-of-age</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr333</td>\n",
       "      <td>sjs457</td>\n",
       "      <td>sl2324</td>\n",
       "      <td>A Round Dozen</td>\n",
       "      <td>1883</td>\n",
       "      <td>Coolidge</td>\n",
       "      <td>Susan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Coolidge_ARoundDozen.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kwy3</td>\n",
       "      <td>cl922</td>\n",
       "      <td>hk627</td>\n",
       "      <td>A Sicillian Romance</td>\n",
       "      <td>1790</td>\n",
       "      <td>Radcliffe</td>\n",
       "      <td>Ann Ward</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>radcliffeann_a_sicillian_romance.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lqz4</td>\n",
       "      <td>gt294</td>\n",
       "      <td>lcc82</td>\n",
       "      <td>Adele Doring at Boarding-School</td>\n",
       "      <td>1921</td>\n",
       "      <td>North</td>\n",
       "      <td>Grace May</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>adele_doring_boarding_school.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yc2669</td>\n",
       "      <td>xf89</td>\n",
       "      <td>wms87</td>\n",
       "      <td>Agnes Grey</td>\n",
       "      <td>1847</td>\n",
       "      <td>Bronte</td>\n",
       "      <td>Anne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bronte_AgnesGrey.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mn454</td>\n",
       "      <td>ar2465</td>\n",
       "      <td>jlp367</td>\n",
       "      <td>An Old-Fashioned Girl</td>\n",
       "      <td>1869</td>\n",
       "      <td>Alcott</td>\n",
       "      <td>Louisa May</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Alcott_AnOld-FashionedGirl.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>jc2739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Side of Paradise</td>\n",
       "      <td>1920</td>\n",
       "      <td>Fitzgerald</td>\n",
       "      <td>F. Scott</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Fitzgerald_ThisSideOfParadise.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>vs339</td>\n",
       "      <td>thh55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To Kill A Mockingbird</td>\n",
       "      <td>1960</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Harper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Lee_ToKillAMockingbird.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>fhh26</td>\n",
       "      <td>gs542</td>\n",
       "      <td>tj256</td>\n",
       "      <td>Twenty Thousand Leagues Under the Sea</td>\n",
       "      <td>1870</td>\n",
       "      <td>Verne</td>\n",
       "      <td>Jules</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Verne_TwentyThousandLeagues.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>dgr73</td>\n",
       "      <td>jlp367</td>\n",
       "      <td>kg428</td>\n",
       "      <td>Uncle Tom's Cabin</td>\n",
       "      <td>1852</td>\n",
       "      <td>Stowe</td>\n",
       "      <td>Harriet Beecher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Stowe_UncleTom_sCabin.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cl2264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Voodoo Planet</td>\n",
       "      <td>1959</td>\n",
       "      <td>Norton</td>\n",
       "      <td>Andre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Norton_VoodooPlanet.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 check_1 check_2                                  title  year  \\\n",
       "0       tr333  sjs457  sl2324                          A Round Dozen  1883   \n",
       "1        kwy3   cl922   hk627                    A Sicillian Romance  1790   \n",
       "2        lqz4   gt294   lcc82        Adele Doring at Boarding-School  1921   \n",
       "3      yc2669    xf89   wms87                             Agnes Grey  1847   \n",
       "4       mn454  ar2465  jlp367                  An Old-Fashioned Girl  1869   \n",
       "..        ...     ...     ...                                    ...   ...   \n",
       "73     jc2739     NaN     NaN                  This Side of Paradise  1920   \n",
       "74      vs339   thh55     NaN                  To Kill A Mockingbird  1960   \n",
       "75      fhh26   gs542   tj256  Twenty Thousand Leagues Under the Sea  1870   \n",
       "76      dgr73  jlp367   kg428                      Uncle Tom's Cabin  1852   \n",
       "77     cl2264     NaN     NaN                          Voodoo Planet  1959   \n",
       "\n",
       "   author1_surname author1_givenname author2_surname author2_givenname  \\\n",
       "0         Coolidge             Susan             NaN               NaN   \n",
       "1        Radcliffe          Ann Ward             NaN               NaN   \n",
       "2            North         Grace May             NaN               NaN   \n",
       "3           Bronte              Anne             NaN               NaN   \n",
       "4           Alcott        Louisa May             NaN               NaN   \n",
       "..             ...               ...             ...               ...   \n",
       "73      Fitzgerald          F. Scott             NaN               NaN   \n",
       "74             Lee            Harper             NaN               NaN   \n",
       "75           Verne             Jules             NaN               NaN   \n",
       "76           Stowe   Harriet Beecher             NaN               NaN   \n",
       "77          Norton             Andre             NaN               NaN   \n",
       "\n",
       "   gender_author1  ... feminist fiction mystery adventure tragedy children  \\\n",
       "0          Female  ...            False   False     False   False    False   \n",
       "1          Female  ...            False   False     False   False    False   \n",
       "2          Female  ...            False   False     False   False     True   \n",
       "3          Female  ...             True   False     False   False    False   \n",
       "4          Female  ...            False   False     False   False     True   \n",
       "..            ...  ...              ...     ...       ...     ...      ...   \n",
       "73           Male  ...            False   False     False   False    False   \n",
       "74         Female  ...            False   False     False   False    False   \n",
       "75           Male  ...            False   False      True   False    False   \n",
       "76         Female  ...            False   False     False   False    False   \n",
       "77         Female  ...            False   False      True   False    False   \n",
       "\n",
       "    regency  manners philosophical coming-of-age  \\\n",
       "0     False    False         False         False   \n",
       "1     False    False         False         False   \n",
       "2     False    False         False         False   \n",
       "3     False     True         False         False   \n",
       "4     False     True         False          True   \n",
       "..      ...      ...           ...           ...   \n",
       "73    False    False         False          True   \n",
       "74    False    False         False         False   \n",
       "75    False    False         False         False   \n",
       "76    False    False         False         False   \n",
       "77    False    False         False         False   \n",
       "\n",
       "                                filename  \n",
       "0               Coolidge_ARoundDozen.txt  \n",
       "1   radcliffeann_a_sicillian_romance.txt  \n",
       "2       adele_doring_boarding_school.txt  \n",
       "3                   Bronte_AgnesGrey.txt  \n",
       "4         Alcott_AnOld-FashionedGirl.txt  \n",
       "..                                   ...  \n",
       "73     Fitzgerald_ThisSideOfParadise.txt  \n",
       "74            Lee_ToKillAMockingbird.txt  \n",
       "75       Verne_TwentyThousandLeagues.txt  \n",
       "76             Stowe_UncleTom_sCabin.txt  \n",
       "77               Norton_VoodooPlanet.txt  \n",
       "\n",
       "[78 rows x 34 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data=testing_data.reset_index(drop=True)\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening book files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get book file names to open\n",
    "training_names = training_data.filename.values\n",
    "testing_names = testing_data.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First book in the training dataset: Leblanc_813.txt\n",
      "First book in the testing dataset: Coolidge_ARoundDozen.txt\n"
     ]
    }
   ],
   "source": [
    "print('First book in the training dataset:',training_names[0])\n",
    "print('First book in the testing dataset:',testing_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1=detective, 0=horror， gold labels\n",
    "y_train=(training_data.detective.values*1).astype('int')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and append training books together\n",
    "training_books=[]\n",
    "for book in training_names:\n",
    "    with open(book, 'r',encoding='utf-8') as f:\n",
    "        file = f.read().replace(\"\\n\", \" \") \n",
    "        training_books.append(file)\n",
    "# open and append testing books together\n",
    "testing_books=[]\n",
    "for book in testing_names:\n",
    "    with open(book, 'r',encoding='utf-8') as f:\n",
    "        file = f.read().replace(\"\\n\", \" \") \n",
    "        testing_books.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Vectorization, stopwords, normalization, standardization, matrix fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "We played around with using stopwords and changing the max_df parameter in the TfidfVectorizer(). We initially did use punctuation as stopwords and compared the matrix with and without using that parameter, and the resulting shape was the same. Given that result, we decided to ignore the stop_words parameter and let the idf weighting take care of the normalization, in addition to L2. We picked L2 normalization over L1 because it L2 downweights the lower features, preserving sparsity, using euclidean distances and helping the explainability of our model which takes in a large about of features across many genres. The max_df =.8 also takes care of the issue of stopwords since it does not consider tokens that are in more than 80% of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct=[]\n",
    "for x in string.punctuation:\n",
    "    punct.append(x)\n",
    "punct.append('--')\n",
    "punct.append('`')\n",
    "punct.append(\"“\")\n",
    "punct.append(\"”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization (+ normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lowercased the words in the books and set up a vectorizer without setting up the max_features parameter. The resulting matrix shows that the training dataset of 80 true detective or  horror novels has a matrix shape of 30,376 features. We also made sure to normalize the matrix because the word count for novels can range from a minimum of 20,000 words to above 100,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (80, 30376)\n"
     ]
    }
   ],
   "source": [
    "# Custom preprocessing to remove escaped characters in input, taken from MP02\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"\\'\", \"'\").replace(\"\\ in\\ form\", \" inform\").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "   # stop_words=punct,\n",
    "    min_df=2, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    use_idf=True, # And this,\n",
    "    #max_features=10000\n",
    ")\n",
    "\n",
    "# Your code here\n",
    "X_train = vectorizer.fit_transform(training_books)\n",
    "print(\"Matrix shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization: Z-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the training data set consists of two genres, and the testing data set consists of multiple genres, we found it especially important to standardize the matrix with Z-scores to make features that seemed important in one genre also relevant across the other genre. It is important to note that because the matrix (X_train) is sparse, we had to set with_mean=False because the function would raise an error. This means that our mean is not scaled to 0, but rather .292. We posted on EdStem privately about this issue more than 24 hrs but were not able to get a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80x30376 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 376031 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-scored l2 mean: 0.292\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "X_train_Z = StandardScaler(with_mean=False).fit_transform(X_train)\n",
    "display(X_train_Z)\n",
    "print('z-scored l2 mean:', round(np.mean(X_train_Z),3))\n",
    "#np.std(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to vectorize the corpus with a variety of different inputs to place into the classifer in addition to the matrix with all the features at 30,376. As seen below, we chose a range of input numbers beginning at 5,000 to 30,376 and saved it to a dictionary `matrix_n` to be ran in combination with the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_n=[]\n",
    "matrix_n= {}\n",
    "feat_n = [5000,10000,15000,17500,20000,22500,25000,30000,30376] # NOTE, get all the features we had to use a value greater than the number of features. So 35000 is actually just all features\n",
    "for x in feat_n:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        encoding='utf-8',\n",
    "        preprocessor=pre_proc,\n",
    "        min_df=2, # Note this\n",
    "        max_df=0.8, # This, too\n",
    "        binary=False,\n",
    "        norm='l2',\n",
    "        use_idf=True, # And this\n",
    "        max_features=x)\n",
    "    vect_n.append(vectorizer)\n",
    "    matrix = vectorizer.fit_transform(training_books)\n",
    "    X_train_Z = StandardScaler(with_mean=False).fit_transform(matrix)\n",
    "    dict_key=str(x)\n",
    "    matrix_n[dict_key] = X_train_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5000': <80x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 187794 stored elements in Compressed Sparse Row format>,\n",
       " '10000': <80x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 274957 stored elements in Compressed Sparse Row format>,\n",
       " '15000': <80x15000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 320917 stored elements in Compressed Sparse Row format>,\n",
       " '17500': <80x17500 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 335934 stored elements in Compressed Sparse Row format>,\n",
       " '20000': <80x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 347813 stored elements in Compressed Sparse Row format>,\n",
       " '22500': <80x22500 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 357000 stored elements in Compressed Sparse Row format>,\n",
       " '25000': <80x25000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 364318 stored elements in Compressed Sparse Row format>,\n",
       " '30000': <80x30000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 375279 stored elements in Compressed Sparse Row format>,\n",
       " '30376': <80x30376 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 376031 stored elements in Compressed Sparse Row format>}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers we were comfortable in interpreting and using for text classification were logistic regression, decision trees, random forest, multinomial naive bayes, and k-nearest neighbors. We decided to not go with Decision Trees because they are prone to overfitting and K-nearest neighbors because it is \"not usually best for real-world cases\" (Lec 12). We were considering going with random forest, but found it too computationally expensive. We decided on logistic regression and naive bayes because they were interpretable, not too computationally expensive, and perform relatively well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After testing these classifiers, we found that with cross validation the classifier that performed the best on our training data is MultinomialNB() with the default parameters, using a matrix that was vectorized with max_features=17,500 (process shown below).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of our simple classifiers\n",
    "# Freebie function to summarize and display classifier scores\n",
    "# source: mp2\n",
    "def compare_scores(scores_dict):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean).style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We played around with different parameters for the multinomial naive bayes classifier but noticed that there was no difference between using fit_prior and not using it. The MultinomialNB() classifier also has a class_prior parameter to indicate the ratio of the labels, but we decided not to use it because we are testing on a dataset that is not horror or detective, so the weights do not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2a05f_row0_col0, #T_2a05f_row0_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2a05f_row0_col2, #T_2a05f_row0_col3, #T_2a05f_row0_col4, #T_2a05f_row0_col5, #T_2a05f_row0_col6, #T_2a05f_row0_col7, #T_2a05f_row1_col0, #T_2a05f_row1_col1, #T_2a05f_row1_col2, #T_2a05f_row1_col3, #T_2a05f_row1_col4, #T_2a05f_row1_col5, #T_2a05f_row1_col6, #T_2a05f_row1_col7 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2a05f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th class=\"col_heading level0 col5\" >test_f1</th>\n",
       "      <th class=\"col_heading level0 col6\" >test_f1_macro</th>\n",
       "      <th class=\"col_heading level0 col7\" >test_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2a05f_level0_row0\" class=\"row_heading level0 row0\" >M NB Default, Alpha=1</th>\n",
       "      <td id=\"T_2a05f_row0_col0\" class=\"data row0 col0\" >0.007405</td>\n",
       "      <td id=\"T_2a05f_row0_col1\" class=\"data row0 col1\" >0.005899</td>\n",
       "      <td id=\"T_2a05f_row0_col2\" class=\"data row0 col2\" >0.887500</td>\n",
       "      <td id=\"T_2a05f_row0_col3\" class=\"data row0 col3\" >0.868095</td>\n",
       "      <td id=\"T_2a05f_row0_col4\" class=\"data row0 col4\" >0.980000</td>\n",
       "      <td id=\"T_2a05f_row0_col5\" class=\"data row0 col5\" >0.917879</td>\n",
       "      <td id=\"T_2a05f_row0_col6\" class=\"data row0 col6\" >0.867273</td>\n",
       "      <td id=\"T_2a05f_row0_col7\" class=\"data row0 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2a05f_level0_row1\" class=\"row_heading level0 row1\" >M NB fit_prior=False</th>\n",
       "      <td id=\"T_2a05f_row1_col0\" class=\"data row1 col0\" >0.006380</td>\n",
       "      <td id=\"T_2a05f_row1_col1\" class=\"data row1 col1\" >0.004178</td>\n",
       "      <td id=\"T_2a05f_row1_col2\" class=\"data row1 col2\" >0.887500</td>\n",
       "      <td id=\"T_2a05f_row1_col3\" class=\"data row1 col3\" >0.868095</td>\n",
       "      <td id=\"T_2a05f_row1_col4\" class=\"data row1 col4\" >0.980000</td>\n",
       "      <td id=\"T_2a05f_row1_col5\" class=\"data row1 col5\" >0.917879</td>\n",
       "      <td id=\"T_2a05f_row1_col6\" class=\"data row1 col6\" >0.867273</td>\n",
       "      <td id=\"T_2a05f_row1_col7\" class=\"data row1 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19eb210ab50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifiers = {\n",
    "    'M NB Default, Alpha=1':MultinomialNB(alpha = 1),\n",
    "    'M NB fit_prior=False':MultinomialNB(fit_prior = False),\n",
    "}\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in nb_classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        nb_classifiers[classifier], # classifier object\n",
    "        X_train_Z, # feature matrix\n",
    "        y_train, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )\n",
    "       \n",
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of testing different MultiNomialNB() parameters: \n",
    "Even though we tried different parameters it did not have an effect on the outcome of any of the scoring for the classifier, so we stuck with the default, and will use the default parameters to over the different sized matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5000)\n",
      "(80, 10000)\n",
      "(80, 15000)\n",
      "(80, 17500)\n",
      "(80, 20000)\n",
      "(80, 22500)\n",
      "(80, 25000)\n",
      "(80, 30000)\n",
      "(80, 30376)\n"
     ]
    }
   ],
   "source": [
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for matrix in matrix_n: \n",
    "    print(matrix_n[matrix].shape)\n",
    "    scores[matrix] = cross_validate( # perform cross-validation\n",
    "        MultinomialNB(alpha = 1), # classifier object\n",
    "        matrix_n[matrix], # feature matrix\n",
    "        y_train, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1e49e_row0_col0, #T_1e49e_row0_col4, #T_1e49e_row1_col4, #T_1e49e_row5_col2, #T_1e49e_row5_col3, #T_1e49e_row5_col4, #T_1e49e_row5_col5, #T_1e49e_row5_col6, #T_1e49e_row5_col7, #T_1e49e_row6_col1, #T_1e49e_row6_col4, #T_1e49e_row7_col4, #T_1e49e_row8_col4 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row0_col1 {\n",
       "  background-color: #8ccd67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row0_col2, #T_1e49e_row0_col7, #T_1e49e_row1_col2, #T_1e49e_row1_col7, #T_1e49e_row4_col2, #T_1e49e_row4_col7, #T_1e49e_row7_col2, #T_1e49e_row7_col7, #T_1e49e_row8_col2, #T_1e49e_row8_col7 {\n",
       "  background-color: #b7e075;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row0_col3, #T_1e49e_row1_col3, #T_1e49e_row7_col3, #T_1e49e_row8_col3 {\n",
       "  background-color: #87cb67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row0_col5, #T_1e49e_row1_col5, #T_1e49e_row7_col5, #T_1e49e_row8_col5 {\n",
       "  background-color: #ecf7a6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row0_col6, #T_1e49e_row1_col6, #T_1e49e_row7_col6, #T_1e49e_row8_col6 {\n",
       "  background-color: #51b35e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row1_col0, #T_1e49e_row2_col6 {\n",
       "  background-color: #fa9857;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row1_col1 {\n",
       "  background-color: #a7d96b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row2_col0 {\n",
       "  background-color: #fee18d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row2_col1 {\n",
       "  background-color: #b30d26;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row2_col2, #T_1e49e_row2_col7, #T_1e49e_row6_col2, #T_1e49e_row6_col7 {\n",
       "  background-color: #fdbf6f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row2_col3 {\n",
       "  background-color: #d83128;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row2_col4, #T_1e49e_row3_col2, #T_1e49e_row3_col3, #T_1e49e_row3_col4, #T_1e49e_row3_col5, #T_1e49e_row3_col6, #T_1e49e_row3_col7, #T_1e49e_row4_col0, #T_1e49e_row4_col1, #T_1e49e_row4_col4 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row2_col5 {\n",
       "  background-color: #fede89;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row3_col0 {\n",
       "  background-color: #98d368;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row3_col1 {\n",
       "  background-color: #7fc866;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row4_col3 {\n",
       "  background-color: #cbe982;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row4_col5 {\n",
       "  background-color: #96d268;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row4_col6 {\n",
       "  background-color: #ddf191;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row5_col0 {\n",
       "  background-color: #fa9b58;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row5_col1 {\n",
       "  background-color: #ea5739;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row6_col0, #T_1e49e_row8_col0 {\n",
       "  background-color: #fff0a6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row6_col3 {\n",
       "  background-color: #fed27f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row6_col5 {\n",
       "  background-color: #fb9d59;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row6_col6 {\n",
       "  background-color: #fee491;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row7_col0 {\n",
       "  background-color: #b9e176;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1e49e_row7_col1 {\n",
       "  background-color: #e24731;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e49e_row8_col1 {\n",
       "  background-color: #c01a27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1e49e_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th class=\"col_heading level0 col5\" >test_f1</th>\n",
       "      <th class=\"col_heading level0 col6\" >test_f1_macro</th>\n",
       "      <th class=\"col_heading level0 col7\" >test_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row0\" class=\"row_heading level0 row0\" >5000</th>\n",
       "      <td id=\"T_1e49e_row0_col0\" class=\"data row0 col0\" >0.003900</td>\n",
       "      <td id=\"T_1e49e_row0_col1\" class=\"data row0 col1\" >0.004929</td>\n",
       "      <td id=\"T_1e49e_row0_col2\" class=\"data row0 col2\" >0.887500</td>\n",
       "      <td id=\"T_1e49e_row0_col3\" class=\"data row0 col3\" >0.868095</td>\n",
       "      <td id=\"T_1e49e_row0_col4\" class=\"data row0 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row0_col5\" class=\"data row0 col5\" >0.917879</td>\n",
       "      <td id=\"T_1e49e_row0_col6\" class=\"data row0 col6\" >0.867273</td>\n",
       "      <td id=\"T_1e49e_row0_col7\" class=\"data row0 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row1\" class=\"row_heading level0 row1\" >10000</th>\n",
       "      <td id=\"T_1e49e_row1_col0\" class=\"data row1 col0\" >0.004622</td>\n",
       "      <td id=\"T_1e49e_row1_col1\" class=\"data row1 col1\" >0.004850</td>\n",
       "      <td id=\"T_1e49e_row1_col2\" class=\"data row1 col2\" >0.887500</td>\n",
       "      <td id=\"T_1e49e_row1_col3\" class=\"data row1 col3\" >0.868095</td>\n",
       "      <td id=\"T_1e49e_row1_col4\" class=\"data row1 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row1_col5\" class=\"data row1 col5\" >0.917879</td>\n",
       "      <td id=\"T_1e49e_row1_col6\" class=\"data row1 col6\" >0.867273</td>\n",
       "      <td id=\"T_1e49e_row1_col7\" class=\"data row1 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row2\" class=\"row_heading level0 row2\" >15000</th>\n",
       "      <td id=\"T_1e49e_row2_col0\" class=\"data row2 col0\" >0.004994</td>\n",
       "      <td id=\"T_1e49e_row2_col1\" class=\"data row2 col1\" >0.003660</td>\n",
       "      <td id=\"T_1e49e_row2_col2\" class=\"data row2 col2\" >0.875000</td>\n",
       "      <td id=\"T_1e49e_row2_col3\" class=\"data row2 col3\" >0.847619</td>\n",
       "      <td id=\"T_1e49e_row2_col4\" class=\"data row2 col4\" >1.000000</td>\n",
       "      <td id=\"T_1e49e_row2_col5\" class=\"data row2 col5\" >0.913636</td>\n",
       "      <td id=\"T_1e49e_row2_col6\" class=\"data row2 col6\" >0.841818</td>\n",
       "      <td id=\"T_1e49e_row2_col7\" class=\"data row2 col7\" >0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row3\" class=\"row_heading level0 row3\" >17500</th>\n",
       "      <td id=\"T_1e49e_row3_col0\" class=\"data row3 col0\" >0.005850</td>\n",
       "      <td id=\"T_1e49e_row3_col1\" class=\"data row3 col1\" >0.004961</td>\n",
       "      <td id=\"T_1e49e_row3_col2\" class=\"data row3 col2\" >0.900000</td>\n",
       "      <td id=\"T_1e49e_row3_col3\" class=\"data row3 col3\" >0.876190</td>\n",
       "      <td id=\"T_1e49e_row3_col4\" class=\"data row3 col4\" >1.000000</td>\n",
       "      <td id=\"T_1e49e_row3_col5\" class=\"data row3 col5\" >0.930303</td>\n",
       "      <td id=\"T_1e49e_row3_col6\" class=\"data row3 col6\" >0.875152</td>\n",
       "      <td id=\"T_1e49e_row3_col7\" class=\"data row3 col7\" >0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row4\" class=\"row_heading level0 row4\" >20000</th>\n",
       "      <td id=\"T_1e49e_row4_col0\" class=\"data row4 col0\" >0.006607</td>\n",
       "      <td id=\"T_1e49e_row4_col1\" class=\"data row4 col1\" >0.005393</td>\n",
       "      <td id=\"T_1e49e_row4_col2\" class=\"data row4 col2\" >0.887500</td>\n",
       "      <td id=\"T_1e49e_row4_col3\" class=\"data row4 col3\" >0.864286</td>\n",
       "      <td id=\"T_1e49e_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_1e49e_row4_col5\" class=\"data row4 col5\" >0.922727</td>\n",
       "      <td id=\"T_1e49e_row4_col6\" class=\"data row4 col6\" >0.856364</td>\n",
       "      <td id=\"T_1e49e_row4_col7\" class=\"data row4 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row5\" class=\"row_heading level0 row5\" >22500</th>\n",
       "      <td id=\"T_1e49e_row5_col0\" class=\"data row5 col0\" >0.004636</td>\n",
       "      <td id=\"T_1e49e_row5_col1\" class=\"data row5 col1\" >0.003902</td>\n",
       "      <td id=\"T_1e49e_row5_col2\" class=\"data row5 col2\" >0.862500</td>\n",
       "      <td id=\"T_1e49e_row5_col3\" class=\"data row5 col3\" >0.844286</td>\n",
       "      <td id=\"T_1e49e_row5_col4\" class=\"data row5 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row5_col5\" class=\"data row5 col5\" >0.902727</td>\n",
       "      <td id=\"T_1e49e_row5_col6\" class=\"data row5 col6\" >0.829697</td>\n",
       "      <td id=\"T_1e49e_row5_col7\" class=\"data row5 col7\" >0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row6\" class=\"row_heading level0 row6\" >25000</th>\n",
       "      <td id=\"T_1e49e_row6_col0\" class=\"data row6 col0\" >0.005120</td>\n",
       "      <td id=\"T_1e49e_row6_col1\" class=\"data row6 col1\" >0.003605</td>\n",
       "      <td id=\"T_1e49e_row6_col2\" class=\"data row6 col2\" >0.875000</td>\n",
       "      <td id=\"T_1e49e_row6_col3\" class=\"data row6 col3\" >0.856190</td>\n",
       "      <td id=\"T_1e49e_row6_col4\" class=\"data row6 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row6_col5\" class=\"data row6 col5\" >0.910303</td>\n",
       "      <td id=\"T_1e49e_row6_col6\" class=\"data row6 col6\" >0.848485</td>\n",
       "      <td id=\"T_1e49e_row6_col7\" class=\"data row6 col7\" >0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row7\" class=\"row_heading level0 row7\" >30000</th>\n",
       "      <td id=\"T_1e49e_row7_col0\" class=\"data row7 col0\" >0.005693</td>\n",
       "      <td id=\"T_1e49e_row7_col1\" class=\"data row7 col1\" >0.003856</td>\n",
       "      <td id=\"T_1e49e_row7_col2\" class=\"data row7 col2\" >0.887500</td>\n",
       "      <td id=\"T_1e49e_row7_col3\" class=\"data row7 col3\" >0.868095</td>\n",
       "      <td id=\"T_1e49e_row7_col4\" class=\"data row7 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row7_col5\" class=\"data row7 col5\" >0.917879</td>\n",
       "      <td id=\"T_1e49e_row7_col6\" class=\"data row7 col6\" >0.867273</td>\n",
       "      <td id=\"T_1e49e_row7_col7\" class=\"data row7 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e49e_level0_row8\" class=\"row_heading level0 row8\" >30376</th>\n",
       "      <td id=\"T_1e49e_row8_col0\" class=\"data row8 col0\" >0.005124</td>\n",
       "      <td id=\"T_1e49e_row8_col1\" class=\"data row8 col1\" >0.003705</td>\n",
       "      <td id=\"T_1e49e_row8_col2\" class=\"data row8 col2\" >0.887500</td>\n",
       "      <td id=\"T_1e49e_row8_col3\" class=\"data row8 col3\" >0.868095</td>\n",
       "      <td id=\"T_1e49e_row8_col4\" class=\"data row8 col4\" >0.980000</td>\n",
       "      <td id=\"T_1e49e_row8_col5\" class=\"data row8 col5\" >0.917879</td>\n",
       "      <td id=\"T_1e49e_row8_col6\" class=\"data row8 col6\" >0.867273</td>\n",
       "      <td id=\"T_1e49e_row8_col7\" class=\"data row8 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19eb63c5280>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of testing MultiNomialNB() over different sized inputs: \n",
    "The default MultinomialNB() with a matrix of max_features=17500 performed the best across all scoring, with an F1 score at .9303. We decided to toy with a more specified range around  17500 features to try to refine this score (in the following cell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_n2=[]\n",
    "matrix_n2= {}\n",
    "feat_n2 = [15000,16000,17000,18000,19000,20000,17500]\n",
    "for x in feat_n2:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        encoding='utf-8',\n",
    "        preprocessor=pre_proc,\n",
    "        min_df=2, # Note this\n",
    "        max_df=0.8, # This, too\n",
    "        binary=False,\n",
    "        norm='l2',\n",
    "        use_idf=True, # And this\n",
    "        max_features=x)\n",
    "    vect_n.append(vectorizer)\n",
    "    matrix = vectorizer.fit_transform(training_books)\n",
    "    X_train_Z = StandardScaler(with_mean=False).fit_transform(matrix)\n",
    "    dict_key=str(x)\n",
    "    matrix_n2[dict_key] = X_train_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = {} # Store cross-validation results in a dictionary\n",
    "for matrix2 in matrix_n2: \n",
    "    scores2[matrix2] = cross_validate( # perform cross-validation\n",
    "        MultinomialNB(), # classifier object\n",
    "        matrix_n2[matrix2], # feature matrix\n",
    "        y_train, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_03d5c_row0_col0, #T_03d5c_row0_col1, #T_03d5c_row0_col4, #T_03d5c_row3_col3, #T_03d5c_row4_col2, #T_03d5c_row4_col3, #T_03d5c_row4_col4, #T_03d5c_row4_col5, #T_03d5c_row4_col6, #T_03d5c_row4_col7, #T_03d5c_row5_col4, #T_03d5c_row6_col2, #T_03d5c_row6_col3, #T_03d5c_row6_col4, #T_03d5c_row6_col5, #T_03d5c_row6_col6, #T_03d5c_row6_col7 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row0_col2, #T_03d5c_row0_col3, #T_03d5c_row0_col6, #T_03d5c_row0_col7, #T_03d5c_row1_col2, #T_03d5c_row1_col4, #T_03d5c_row1_col5, #T_03d5c_row1_col7, #T_03d5c_row2_col2, #T_03d5c_row2_col4, #T_03d5c_row2_col5, #T_03d5c_row2_col7, #T_03d5c_row3_col1, #T_03d5c_row3_col4, #T_03d5c_row6_col0 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row0_col5 {\n",
       "  background-color: #d93429;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row1_col0 {\n",
       "  background-color: #b5df74;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row1_col1 {\n",
       "  background-color: #84ca66;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row1_col3, #T_03d5c_row2_col3, #T_03d5c_row5_col3 {\n",
       "  background-color: #dff293;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row1_col6, #T_03d5c_row2_col6 {\n",
       "  background-color: #c01a27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row2_col0 {\n",
       "  background-color: #feda86;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row2_col1 {\n",
       "  background-color: #fdbb6c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row3_col0 {\n",
       "  background-color: #cc2627;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row3_col2, #T_03d5c_row3_col7, #T_03d5c_row5_col2, #T_03d5c_row5_col7 {\n",
       "  background-color: #fffebe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row3_col5 {\n",
       "  background-color: #fee18d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row3_col6 {\n",
       "  background-color: #cfeb85;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row4_col0 {\n",
       "  background-color: #c41e27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row4_col1 {\n",
       "  background-color: #a70226;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row5_col0 {\n",
       "  background-color: #e54e35;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row5_col1 {\n",
       "  background-color: #c21c27;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_03d5c_row5_col5 {\n",
       "  background-color: #daf08d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row5_col6 {\n",
       "  background-color: #feeb9d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_03d5c_row6_col1 {\n",
       "  background-color: #b10b26;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_03d5c_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th class=\"col_heading level0 col5\" >test_f1</th>\n",
       "      <th class=\"col_heading level0 col6\" >test_f1_macro</th>\n",
       "      <th class=\"col_heading level0 col7\" >test_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row0\" class=\"row_heading level0 row0\" >15000</th>\n",
       "      <td id=\"T_03d5c_row0_col0\" class=\"data row0 col0\" >0.007039</td>\n",
       "      <td id=\"T_03d5c_row0_col1\" class=\"data row0 col1\" >0.006165</td>\n",
       "      <td id=\"T_03d5c_row0_col2\" class=\"data row0 col2\" >0.875000</td>\n",
       "      <td id=\"T_03d5c_row0_col3\" class=\"data row0 col3\" >0.847619</td>\n",
       "      <td id=\"T_03d5c_row0_col4\" class=\"data row0 col4\" >1.000000</td>\n",
       "      <td id=\"T_03d5c_row0_col5\" class=\"data row0 col5\" >0.913636</td>\n",
       "      <td id=\"T_03d5c_row0_col6\" class=\"data row0 col6\" >0.841818</td>\n",
       "      <td id=\"T_03d5c_row0_col7\" class=\"data row0 col7\" >0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row1\" class=\"row_heading level0 row1\" >16000</th>\n",
       "      <td id=\"T_03d5c_row1_col0\" class=\"data row1 col0\" >0.006102</td>\n",
       "      <td id=\"T_03d5c_row1_col1\" class=\"data row1 col1\" >0.005514</td>\n",
       "      <td id=\"T_03d5c_row1_col2\" class=\"data row1 col2\" >0.875000</td>\n",
       "      <td id=\"T_03d5c_row1_col3\" class=\"data row1 col3\" >0.864286</td>\n",
       "      <td id=\"T_03d5c_row1_col4\" class=\"data row1 col4\" >0.980000</td>\n",
       "      <td id=\"T_03d5c_row1_col5\" class=\"data row1 col5\" >0.911616</td>\n",
       "      <td id=\"T_03d5c_row1_col6\" class=\"data row1 col6\" >0.843665</td>\n",
       "      <td id=\"T_03d5c_row1_col7\" class=\"data row1 col7\" >0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row2\" class=\"row_heading level0 row2\" >17000</th>\n",
       "      <td id=\"T_03d5c_row2_col0\" class=\"data row2 col0\" >0.005307</td>\n",
       "      <td id=\"T_03d5c_row2_col1\" class=\"data row2 col1\" >0.004387</td>\n",
       "      <td id=\"T_03d5c_row2_col2\" class=\"data row2 col2\" >0.875000</td>\n",
       "      <td id=\"T_03d5c_row2_col3\" class=\"data row2 col3\" >0.864286</td>\n",
       "      <td id=\"T_03d5c_row2_col4\" class=\"data row2 col4\" >0.980000</td>\n",
       "      <td id=\"T_03d5c_row2_col5\" class=\"data row2 col5\" >0.911616</td>\n",
       "      <td id=\"T_03d5c_row2_col6\" class=\"data row2 col6\" >0.843665</td>\n",
       "      <td id=\"T_03d5c_row2_col7\" class=\"data row2 col7\" >0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row3\" class=\"row_heading level0 row3\" >18000</th>\n",
       "      <td id=\"T_03d5c_row3_col0\" class=\"data row3 col0\" >0.004437</td>\n",
       "      <td id=\"T_03d5c_row3_col1\" class=\"data row3 col1\" >0.003530</td>\n",
       "      <td id=\"T_03d5c_row3_col2\" class=\"data row3 col2\" >0.887500</td>\n",
       "      <td id=\"T_03d5c_row3_col3\" class=\"data row3 col3\" >0.876190</td>\n",
       "      <td id=\"T_03d5c_row3_col4\" class=\"data row3 col4\" >0.980000</td>\n",
       "      <td id=\"T_03d5c_row3_col5\" class=\"data row3 col5\" >0.919192</td>\n",
       "      <td id=\"T_03d5c_row3_col6\" class=\"data row3 col6\" >0.862453</td>\n",
       "      <td id=\"T_03d5c_row3_col7\" class=\"data row3 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row4\" class=\"row_heading level0 row4\" >19000</th>\n",
       "      <td id=\"T_03d5c_row4_col0\" class=\"data row4 col0\" >0.004389</td>\n",
       "      <td id=\"T_03d5c_row4_col1\" class=\"data row4 col1\" >0.003547</td>\n",
       "      <td id=\"T_03d5c_row4_col2\" class=\"data row4 col2\" >0.900000</td>\n",
       "      <td id=\"T_03d5c_row4_col3\" class=\"data row4 col3\" >0.876190</td>\n",
       "      <td id=\"T_03d5c_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_03d5c_row4_col5\" class=\"data row4 col5\" >0.930303</td>\n",
       "      <td id=\"T_03d5c_row4_col6\" class=\"data row4 col6\" >0.875152</td>\n",
       "      <td id=\"T_03d5c_row4_col7\" class=\"data row4 col7\" >0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row5\" class=\"row_heading level0 row5\" >20000</th>\n",
       "      <td id=\"T_03d5c_row5_col0\" class=\"data row5 col0\" >0.004630</td>\n",
       "      <td id=\"T_03d5c_row5_col1\" class=\"data row5 col1\" >0.003686</td>\n",
       "      <td id=\"T_03d5c_row5_col2\" class=\"data row5 col2\" >0.887500</td>\n",
       "      <td id=\"T_03d5c_row5_col3\" class=\"data row5 col3\" >0.864286</td>\n",
       "      <td id=\"T_03d5c_row5_col4\" class=\"data row5 col4\" >1.000000</td>\n",
       "      <td id=\"T_03d5c_row5_col5\" class=\"data row5 col5\" >0.922727</td>\n",
       "      <td id=\"T_03d5c_row5_col6\" class=\"data row5 col6\" >0.856364</td>\n",
       "      <td id=\"T_03d5c_row5_col7\" class=\"data row5 col7\" >0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03d5c_level0_row6\" class=\"row_heading level0 row6\" >17500</th>\n",
       "      <td id=\"T_03d5c_row6_col0\" class=\"data row6 col0\" >0.004210</td>\n",
       "      <td id=\"T_03d5c_row6_col1\" class=\"data row6 col1\" >0.003601</td>\n",
       "      <td id=\"T_03d5c_row6_col2\" class=\"data row6 col2\" >0.900000</td>\n",
       "      <td id=\"T_03d5c_row6_col3\" class=\"data row6 col3\" >0.876190</td>\n",
       "      <td id=\"T_03d5c_row6_col4\" class=\"data row6 col4\" >1.000000</td>\n",
       "      <td id=\"T_03d5c_row6_col5\" class=\"data row6 col5\" >0.930303</td>\n",
       "      <td id=\"T_03d5c_row6_col6\" class=\"data row6 col6\" >0.875152</td>\n",
       "      <td id=\"T_03d5c_row6_col7\" class=\"data row6 col7\" >0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19eb794e070>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_scores(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of testing MultiNomialNB() over different sized inputs closer to 17,500 features:\n",
    "17,500 features still performed the best, tied with 19,000 features. **The best MultiNomialNB() classifier is with the default parameters and a matrix with 17,500 inputs at an F1 score of .9303.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Like MultinomialNB(), we also tested a variety of parameters, the relevant one being max_iter(), as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f9c28_row0_col0, #T_f9c28_row0_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f9c28_row0_col2, #T_f9c28_row0_col3, #T_f9c28_row0_col4, #T_f9c28_row0_col5, #T_f9c28_row0_col6, #T_f9c28_row0_col7, #T_f9c28_row1_col1, #T_f9c28_row1_col2, #T_f9c28_row1_col3, #T_f9c28_row1_col4, #T_f9c28_row1_col5, #T_f9c28_row1_col6, #T_f9c28_row1_col7, #T_f9c28_row2_col2, #T_f9c28_row2_col3, #T_f9c28_row2_col4, #T_f9c28_row2_col5, #T_f9c28_row2_col6, #T_f9c28_row2_col7, #T_f9c28_row3_col0, #T_f9c28_row3_col2, #T_f9c28_row3_col3, #T_f9c28_row3_col4, #T_f9c28_row3_col5, #T_f9c28_row3_col6, #T_f9c28_row3_col7, #T_f9c28_row4_col2, #T_f9c28_row4_col3, #T_f9c28_row4_col4, #T_f9c28_row4_col5, #T_f9c28_row4_col6, #T_f9c28_row4_col7 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f9c28_row1_col0 {\n",
       "  background-color: #5db961;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f9c28_row2_col0 {\n",
       "  background-color: #f8fcb6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f9c28_row2_col1 {\n",
       "  background-color: #da362a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f9c28_row3_col1 {\n",
       "  background-color: #17934e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f9c28_row4_col0 {\n",
       "  background-color: #fff1a8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f9c28_row4_col1 {\n",
       "  background-color: #fff6b0;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f9c28_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th class=\"col_heading level0 col5\" >test_f1</th>\n",
       "      <th class=\"col_heading level0 col6\" >test_f1_macro</th>\n",
       "      <th class=\"col_heading level0 col7\" >test_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c28_level0_row0\" class=\"row_heading level0 row0\" >log1</th>\n",
       "      <td id=\"T_f9c28_row0_col0\" class=\"data row0 col0\" >0.218998</td>\n",
       "      <td id=\"T_f9c28_row0_col1\" class=\"data row0 col1\" >0.004959</td>\n",
       "      <td id=\"T_f9c28_row0_col2\" class=\"data row0 col2\" >0.800000</td>\n",
       "      <td id=\"T_f9c28_row0_col3\" class=\"data row0 col3\" >0.786310</td>\n",
       "      <td id=\"T_f9c28_row0_col4\" class=\"data row0 col4\" >0.983333</td>\n",
       "      <td id=\"T_f9c28_row0_col5\" class=\"data row0 col5\" >0.866317</td>\n",
       "      <td id=\"T_f9c28_row0_col6\" class=\"data row0 col6\" >0.728159</td>\n",
       "      <td id=\"T_f9c28_row0_col7\" class=\"data row0 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c28_level0_row1\" class=\"row_heading level0 row1\" >log2</th>\n",
       "      <td id=\"T_f9c28_row1_col0\" class=\"data row1 col0\" >0.215387</td>\n",
       "      <td id=\"T_f9c28_row1_col1\" class=\"data row1 col1\" >0.004337</td>\n",
       "      <td id=\"T_f9c28_row1_col2\" class=\"data row1 col2\" >0.800000</td>\n",
       "      <td id=\"T_f9c28_row1_col3\" class=\"data row1 col3\" >0.786310</td>\n",
       "      <td id=\"T_f9c28_row1_col4\" class=\"data row1 col4\" >0.983333</td>\n",
       "      <td id=\"T_f9c28_row1_col5\" class=\"data row1 col5\" >0.866317</td>\n",
       "      <td id=\"T_f9c28_row1_col6\" class=\"data row1 col6\" >0.728159</td>\n",
       "      <td id=\"T_f9c28_row1_col7\" class=\"data row1 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c28_level0_row2\" class=\"row_heading level0 row2\" >log3</th>\n",
       "      <td id=\"T_f9c28_row2_col0\" class=\"data row2 col0\" >0.209780</td>\n",
       "      <td id=\"T_f9c28_row2_col1\" class=\"data row2 col1\" >0.004406</td>\n",
       "      <td id=\"T_f9c28_row2_col2\" class=\"data row2 col2\" >0.800000</td>\n",
       "      <td id=\"T_f9c28_row2_col3\" class=\"data row2 col3\" >0.786310</td>\n",
       "      <td id=\"T_f9c28_row2_col4\" class=\"data row2 col4\" >0.983333</td>\n",
       "      <td id=\"T_f9c28_row2_col5\" class=\"data row2 col5\" >0.866317</td>\n",
       "      <td id=\"T_f9c28_row2_col6\" class=\"data row2 col6\" >0.728159</td>\n",
       "      <td id=\"T_f9c28_row2_col7\" class=\"data row2 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c28_level0_row3\" class=\"row_heading level0 row3\" >log4</th>\n",
       "      <td id=\"T_f9c28_row3_col0\" class=\"data row3 col0\" >0.199911</td>\n",
       "      <td id=\"T_f9c28_row3_col1\" class=\"data row3 col1\" >0.004901</td>\n",
       "      <td id=\"T_f9c28_row3_col2\" class=\"data row3 col2\" >0.800000</td>\n",
       "      <td id=\"T_f9c28_row3_col3\" class=\"data row3 col3\" >0.786310</td>\n",
       "      <td id=\"T_f9c28_row3_col4\" class=\"data row3 col4\" >0.983333</td>\n",
       "      <td id=\"T_f9c28_row3_col5\" class=\"data row3 col5\" >0.866317</td>\n",
       "      <td id=\"T_f9c28_row3_col6\" class=\"data row3 col6\" >0.728159</td>\n",
       "      <td id=\"T_f9c28_row3_col7\" class=\"data row3 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c28_level0_row4\" class=\"row_heading level0 row4\" >log5</th>\n",
       "      <td id=\"T_f9c28_row4_col0\" class=\"data row4 col0\" >0.208586</td>\n",
       "      <td id=\"T_f9c28_row4_col1\" class=\"data row4 col1\" >0.004630</td>\n",
       "      <td id=\"T_f9c28_row4_col2\" class=\"data row4 col2\" >0.800000</td>\n",
       "      <td id=\"T_f9c28_row4_col3\" class=\"data row4 col3\" >0.786310</td>\n",
       "      <td id=\"T_f9c28_row4_col4\" class=\"data row4 col4\" >0.983333</td>\n",
       "      <td id=\"T_f9c28_row4_col5\" class=\"data row4 col5\" >0.866317</td>\n",
       "      <td id=\"T_f9c28_row4_col6\" class=\"data row4 col6\" >0.728159</td>\n",
       "      <td id=\"T_f9c28_row4_col7\" class=\"data row4 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19ea860c970>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit log reg classifier\n",
    "log_classifiers = {\n",
    "    'log1':LogisticRegression(),\n",
    "    'log2':LogisticRegression(max_iter = 1000),\n",
    "    'log3':LogisticRegression(max_iter = 5000),\n",
    "    'log4':LogisticRegression(max_iter = 10000),\n",
    "    'log5':LogisticRegression(max_iter = 50000)\n",
    "}\n",
    "\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in log_classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        log_classifiers[classifier], # classifier object\n",
    "        X_train_Z, # feature matrix\n",
    "        y_train, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )\n",
    "       \n",
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of testing LogisticRegression() with different max_iter:\n",
    "Like the naive bayes classifier, we found that toying with the parameters did nothing to change the F1 score or any of the scoring. We decided to stick with the default parameters in testing over different sized matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5000)\n",
      "(80, 10000)\n",
      "(80, 15000)\n",
      "(80, 17500)\n",
      "(80, 20000)\n",
      "(80, 22500)\n",
      "(80, 25000)\n",
      "(80, 30000)\n",
      "(80, 30376)\n"
     ]
    }
   ],
   "source": [
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for matrix in matrix_n: \n",
    "    print(matrix_n[matrix].shape)\n",
    "    scores[matrix] = cross_validate( # perform cross-validation\n",
    "        LogisticRegression(), # classifier object\n",
    "        matrix_n[matrix], # feature matrix\n",
    "        y_train, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1', 'f1_macro', 'f1_micro'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c4b9a_row0_col0, #T_c4b9a_row0_col1, #T_c4b9a_row1_col4, #T_c4b9a_row7_col2, #T_c4b9a_row7_col3, #T_c4b9a_row7_col5, #T_c4b9a_row7_col6, #T_c4b9a_row7_col7, #T_c4b9a_row8_col2, #T_c4b9a_row8_col3, #T_c4b9a_row8_col5, #T_c4b9a_row8_col6, #T_c4b9a_row8_col7 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row0_col2, #T_c4b9a_row0_col3, #T_c4b9a_row0_col5, #T_c4b9a_row0_col6, #T_c4b9a_row0_col7, #T_c4b9a_row1_col1, #T_c4b9a_row5_col4, #T_c4b9a_row6_col4, #T_c4b9a_row7_col4, #T_c4b9a_row8_col0, #T_c4b9a_row8_col4 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row0_col4 {\n",
       "  background-color: #fff1a8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row1_col0 {\n",
       "  background-color: #ef633f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row1_col2, #T_c4b9a_row1_col7, #T_c4b9a_row2_col2, #T_c4b9a_row2_col7 {\n",
       "  background-color: #fee999;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row1_col3 {\n",
       "  background-color: #f1f9ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row1_col5 {\n",
       "  background-color: #fdaf62;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row1_col6 {\n",
       "  background-color: #ddf191;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col0 {\n",
       "  background-color: #fecc7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col1 {\n",
       "  background-color: #84ca66;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col3 {\n",
       "  background-color: #fff7b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col4, #T_c4b9a_row3_col4, #T_c4b9a_row4_col4 {\n",
       "  background-color: #eef8a8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col5 {\n",
       "  background-color: #fede89;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row2_col6 {\n",
       "  background-color: #fffcba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row3_col0 {\n",
       "  background-color: #fffbb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row3_col1 {\n",
       "  background-color: #91d068;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row3_col2, #T_c4b9a_row3_col7, #T_c4b9a_row5_col2, #T_c4b9a_row5_col7 {\n",
       "  background-color: #fca55d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row3_col3 {\n",
       "  background-color: #fdc171;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row3_col5, #T_c4b9a_row4_col3, #T_c4b9a_row5_col3 {\n",
       "  background-color: #f7844e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row3_col6 {\n",
       "  background-color: #fed27f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row4_col0 {\n",
       "  background-color: #f8fcb6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row4_col1 {\n",
       "  background-color: #0a7b41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row4_col2, #T_c4b9a_row4_col7, #T_c4b9a_row6_col2, #T_c4b9a_row6_col7 {\n",
       "  background-color: #e34933;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row4_col5, #T_c4b9a_row6_col3 {\n",
       "  background-color: #d83128;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row4_col6 {\n",
       "  background-color: #f46d43;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row5_col0 {\n",
       "  background-color: #c9e881;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row5_col1 {\n",
       "  background-color: #05713c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row5_col5 {\n",
       "  background-color: #fcaa5f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row5_col6 {\n",
       "  background-color: #fca85e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row6_col0 {\n",
       "  background-color: #b5df74;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row6_col1 {\n",
       "  background-color: #fbfdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c4b9a_row6_col5 {\n",
       "  background-color: #e14430;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row6_col6 {\n",
       "  background-color: #ec5c3b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row7_col0 {\n",
       "  background-color: #0b7d42;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row7_col1 {\n",
       "  background-color: #5ab760;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c4b9a_row8_col1 {\n",
       "  background-color: #279f53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c4b9a_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th class=\"col_heading level0 col5\" >test_f1</th>\n",
       "      <th class=\"col_heading level0 col6\" >test_f1_macro</th>\n",
       "      <th class=\"col_heading level0 col7\" >test_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row0\" class=\"row_heading level0 row0\" >5000</th>\n",
       "      <td id=\"T_c4b9a_row0_col0\" class=\"data row0 col0\" >0.075388</td>\n",
       "      <td id=\"T_c4b9a_row0_col1\" class=\"data row0 col1\" >0.003987</td>\n",
       "      <td id=\"T_c4b9a_row0_col2\" class=\"data row0 col2\" >0.862500</td>\n",
       "      <td id=\"T_c4b9a_row0_col3\" class=\"data row0 col3\" >0.844286</td>\n",
       "      <td id=\"T_c4b9a_row0_col4\" class=\"data row0 col4\" >0.980000</td>\n",
       "      <td id=\"T_c4b9a_row0_col5\" class=\"data row0 col5\" >0.902727</td>\n",
       "      <td id=\"T_c4b9a_row0_col6\" class=\"data row0 col6\" >0.829697</td>\n",
       "      <td id=\"T_c4b9a_row0_col7\" class=\"data row0 col7\" >0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row1\" class=\"row_heading level0 row1\" >10000</th>\n",
       "      <td id=\"T_c4b9a_row1_col0\" class=\"data row1 col0\" >0.131371</td>\n",
       "      <td id=\"T_c4b9a_row1_col1\" class=\"data row1 col1\" >0.004674</td>\n",
       "      <td id=\"T_c4b9a_row1_col2\" class=\"data row1 col2\" >0.812500</td>\n",
       "      <td id=\"T_c4b9a_row1_col3\" class=\"data row1 col3\" >0.803810</td>\n",
       "      <td id=\"T_c4b9a_row1_col4\" class=\"data row1 col4\" >0.963333</td>\n",
       "      <td id=\"T_c4b9a_row1_col5\" class=\"data row1 col5\" >0.869394</td>\n",
       "      <td id=\"T_c4b9a_row1_col6\" class=\"data row1 col6\" >0.763030</td>\n",
       "      <td id=\"T_c4b9a_row1_col7\" class=\"data row1 col7\" >0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row2\" class=\"row_heading level0 row2\" >15000</th>\n",
       "      <td id=\"T_c4b9a_row2_col0\" class=\"data row2 col0\" >0.184096</td>\n",
       "      <td id=\"T_c4b9a_row2_col1\" class=\"data row2 col1\" >0.004504</td>\n",
       "      <td id=\"T_c4b9a_row2_col2\" class=\"data row2 col2\" >0.812500</td>\n",
       "      <td id=\"T_c4b9a_row2_col3\" class=\"data row2 col3\" >0.798214</td>\n",
       "      <td id=\"T_c4b9a_row2_col4\" class=\"data row2 col4\" >0.983333</td>\n",
       "      <td id=\"T_c4b9a_row2_col5\" class=\"data row2 col5\" >0.873893</td>\n",
       "      <td id=\"T_c4b9a_row2_col6\" class=\"data row2 col6\" >0.746946</td>\n",
       "      <td id=\"T_c4b9a_row2_col7\" class=\"data row2 col7\" >0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row3\" class=\"row_heading level0 row3\" >17500</th>\n",
       "      <td id=\"T_c4b9a_row3_col0\" class=\"data row3 col0\" >0.221013</td>\n",
       "      <td id=\"T_c4b9a_row3_col1\" class=\"data row3 col1\" >0.004489</td>\n",
       "      <td id=\"T_c4b9a_row3_col2\" class=\"data row3 col2\" >0.800000</td>\n",
       "      <td id=\"T_c4b9a_row3_col3\" class=\"data row3 col3\" >0.786310</td>\n",
       "      <td id=\"T_c4b9a_row3_col4\" class=\"data row3 col4\" >0.983333</td>\n",
       "      <td id=\"T_c4b9a_row3_col5\" class=\"data row3 col5\" >0.866317</td>\n",
       "      <td id=\"T_c4b9a_row3_col6\" class=\"data row3 col6\" >0.728159</td>\n",
       "      <td id=\"T_c4b9a_row3_col7\" class=\"data row3 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row4\" class=\"row_heading level0 row4\" >20000</th>\n",
       "      <td id=\"T_c4b9a_row4_col0\" class=\"data row4 col0\" >0.230911</td>\n",
       "      <td id=\"T_c4b9a_row4_col1\" class=\"data row4 col1\" >0.004647</td>\n",
       "      <td id=\"T_c4b9a_row4_col2\" class=\"data row4 col2\" >0.787500</td>\n",
       "      <td id=\"T_c4b9a_row4_col3\" class=\"data row4 col3\" >0.777381</td>\n",
       "      <td id=\"T_c4b9a_row4_col4\" class=\"data row4 col4\" >0.983333</td>\n",
       "      <td id=\"T_c4b9a_row4_col5\" class=\"data row4 col5\" >0.859907</td>\n",
       "      <td id=\"T_c4b9a_row4_col6\" class=\"data row4 col6\" >0.699953</td>\n",
       "      <td id=\"T_c4b9a_row4_col7\" class=\"data row4 col7\" >0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row5\" class=\"row_heading level0 row5\" >22500</th>\n",
       "      <td id=\"T_c4b9a_row5_col0\" class=\"data row5 col0\" >0.264389</td>\n",
       "      <td id=\"T_c4b9a_row5_col1\" class=\"data row5 col1\" >0.004660</td>\n",
       "      <td id=\"T_c4b9a_row5_col2\" class=\"data row5 col2\" >0.800000</td>\n",
       "      <td id=\"T_c4b9a_row5_col3\" class=\"data row5 col3\" >0.777381</td>\n",
       "      <td id=\"T_c4b9a_row5_col4\" class=\"data row5 col4\" >1.000000</td>\n",
       "      <td id=\"T_c4b9a_row5_col5\" class=\"data row5 col5\" >0.868998</td>\n",
       "      <td id=\"T_c4b9a_row5_col6\" class=\"data row5 col6\" >0.714499</td>\n",
       "      <td id=\"T_c4b9a_row5_col7\" class=\"data row5 col7\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row6\" class=\"row_heading level0 row6\" >25000</th>\n",
       "      <td id=\"T_c4b9a_row6_col0\" class=\"data row6 col0\" >0.275974</td>\n",
       "      <td id=\"T_c4b9a_row6_col1\" class=\"data row6 col1\" >0.004339</td>\n",
       "      <td id=\"T_c4b9a_row6_col2\" class=\"data row6 col2\" >0.787500</td>\n",
       "      <td id=\"T_c4b9a_row6_col3\" class=\"data row6 col3\" >0.765476</td>\n",
       "      <td id=\"T_c4b9a_row6_col4\" class=\"data row6 col4\" >1.000000</td>\n",
       "      <td id=\"T_c4b9a_row6_col5\" class=\"data row6 col5\" >0.861422</td>\n",
       "      <td id=\"T_c4b9a_row6_col6\" class=\"data row6 col6\" >0.695711</td>\n",
       "      <td id=\"T_c4b9a_row6_col7\" class=\"data row6 col7\" >0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row7\" class=\"row_heading level0 row7\" >30000</th>\n",
       "      <td id=\"T_c4b9a_row7_col0\" class=\"data row7 col0\" >0.361583</td>\n",
       "      <td id=\"T_c4b9a_row7_col1\" class=\"data row7 col1\" >0.004548</td>\n",
       "      <td id=\"T_c4b9a_row7_col2\" class=\"data row7 col2\" >0.775000</td>\n",
       "      <td id=\"T_c4b9a_row7_col3\" class=\"data row7 col3\" >0.756548</td>\n",
       "      <td id=\"T_c4b9a_row7_col4\" class=\"data row7 col4\" >1.000000</td>\n",
       "      <td id=\"T_c4b9a_row7_col5\" class=\"data row7 col5\" >0.855012</td>\n",
       "      <td id=\"T_c4b9a_row7_col6\" class=\"data row7 col6\" >0.667506</td>\n",
       "      <td id=\"T_c4b9a_row7_col7\" class=\"data row7 col7\" >0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4b9a_level0_row8\" class=\"row_heading level0 row8\" >30376</th>\n",
       "      <td id=\"T_c4b9a_row8_col0\" class=\"data row8 col0\" >0.375540</td>\n",
       "      <td id=\"T_c4b9a_row8_col1\" class=\"data row8 col1\" >0.004593</td>\n",
       "      <td id=\"T_c4b9a_row8_col2\" class=\"data row8 col2\" >0.775000</td>\n",
       "      <td id=\"T_c4b9a_row8_col3\" class=\"data row8 col3\" >0.756548</td>\n",
       "      <td id=\"T_c4b9a_row8_col4\" class=\"data row8 col4\" >1.000000</td>\n",
       "      <td id=\"T_c4b9a_row8_col5\" class=\"data row8 col5\" >0.855012</td>\n",
       "      <td id=\"T_c4b9a_row8_col6\" class=\"data row8 col6\" >0.667506</td>\n",
       "      <td id=\"T_c4b9a_row8_col7\" class=\"data row8 col7\" >0.775000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19eb64c5430>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of testing LogisticRegression() over different sized inputs: \n",
    "The default LogisticRegression() with a matrix of max_features=5000 performed the best across all scoring, with an F1 score at .9027. We decided to not test LogisticRegression() further because 5000 features seemed too little for our purpose, and refining this model would mean going below that number. We decided to go with the MultinomialNB() classifier instead, which perfomed about 3% better with an F1 score of about .9303."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IV. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to see what specific features that were weighted most heavily in deciding if a model was detective or horror. Additionally, extracting these tokens is important to see if our model is using tokens that it shouldn't when doing classification, such as characters' names, articles, or punctuation. If the latter was the case, then we would go back and adjust our pre-processing for our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important words horror books:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['content', 'reality', 'closer', 'flesh', 'yield', 'choice',\n",
       "       'attend', 'occur', 'struggled', 'whence', 'music', 'tender',\n",
       "       'storm', 'solemn', 'mass', 'distress', 'dreaming', 'kissed',\n",
       "       'flow', 'equal', 'trembled', 'gladly', 'wandering', 'travel',\n",
       "       'reasonable', 'strongly', 'endless', 'glory', 'motion',\n",
       "       'exhausted', 'provided', 'breathing', 'suffer', 'changes',\n",
       "       'contrast', 'sleeping', 'composed', 'confusion', 'encourage',\n",
       "       'considering', 'strain', 'wave', 'dreams', 'justified', 'animal',\n",
       "       'tortured', 'branches', 'dignity', 'turns', 'treated'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most important words detective books:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['marry', 'morrow', 'suspicions', 'famous', 'aroused', 'suspicious',\n",
       "       'warn', 'valuable', 'stolen', 'smoking', 'clue', 'profession',\n",
       "       'conceal', 'arrested', 'hunting', 'interests', 'advised',\n",
       "       'reasonable', 'impatiently', 'finds', 'objection', 'tragedy',\n",
       "       'prefer', 'liberty', 'exclamation', 'picking', 'begged', 'mud',\n",
       "       'visitors', 'tells', 'bending', 'absurd', 'gets', 'card',\n",
       "       'considering', 'propose', 'disappearance', 'lunch', 'crushed',\n",
       "       'consulted', 'investigation', 'gesture', 'employ', 'informed',\n",
       "       'appreciate', 'cigar', 'detective', 'blame', 'replaced',\n",
       "       'reputation'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#source: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes\n",
    "feat_impNB=MultinomialNB()\n",
    "s=feat_impNB.fit(matrix_n2['17500'],y_train)\n",
    "\n",
    "neg_class_prob_sorted = feat_impNB.feature_log_prob_[0, :].argsort()[::-1]\n",
    "pos_class_prob_sorted = feat_impNB.feature_log_prob_[1, :].argsort()[::-1]\n",
    "\n",
    "\n",
    "print(\"The most important words horror books:\\n\")\n",
    "display(np.take(vectorizer.get_feature_names_out(), neg_class_prob_sorted[:50]))\n",
    "print(\"\\nThe most important words detective books:\\n\")\n",
    "display(np.take(vectorizer.get_feature_names_out(), pos_class_prob_sorted[:50])) #??? detective=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations of looking at the heaviest-weighted tokens for detective and horror\n",
    "Overall, we can observe that the top tokens for each genre do not have overlap. Additionally, these tokens do not seem that strange for their genre, with detective using words like \"suspicions\" and \"stolen\" and horror using words like \"distress\" and \"trembled.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Testing/Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we fitted our best model, we can use the testing data of other novels and predict them as \"horror\" or \"detective\" using the classifier. It is important to note that we cannot check the accuracy of this fitting or score it because our testing_data does not consist of true labeled detective or horror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "        encoding='utf-8',\n",
    "        preprocessor=pre_proc,\n",
    "        min_df=2, # Note this\n",
    "        max_df=0.8, # This, too\n",
    "        binary=False,\n",
    "        norm='l2',\n",
    "        use_idf=True, # And this\n",
    "        max_features=17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.fit_transform(testing_books)\n",
    "y_test = MultinomialNB().fit(matrix_n['17500'], y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detective=1, Horror=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing books predicted Detective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the n=78 testing dataset, the following 68 novels were predicted as detective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books predicted detective:\n",
      "68\n",
      "Book titles of other genres predicted detective:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['A Round Dozen', 'A Sicillian Romance',\n",
       "       'Adele Doring at Boarding-School', 'Agnes Grey',\n",
       "       'An Old-Fashioned Girl', 'Anna Karenina', 'Don Quixote', 'Emma',\n",
       "       'Flatland', 'Key Out of Time',\n",
       "       \"Little Men: Life at Plumfield With Jo's Boys\", 'Little Women',\n",
       "       'Mansfield Park', 'Mathilda', 'Micah Clarke', 'Middlemarch',\n",
       "       'Mizora: A Prophecy', 'Moods', 'Mr. Standfast', 'Night and Day',\n",
       "       \"Nobody's Girl\", 'Our Mutual Friend', 'Persuasion', 'Plague Ship',\n",
       "       'Pride and Prejudice', 'Rainbow Valley', 'Sense and Sensibility',\n",
       "       'Sense and Sensibility', 'Shirley', 'Silas Marner', 'Star Hunter',\n",
       "       'Star of India', 'Storm over Warlock', 'Summer',\n",
       "       'The Age of Innocence', 'The Beautiful and Damned', 'The Bell Jar',\n",
       "       'The Best Made Plans', 'The Betrothed', 'The Colors of Space',\n",
       "       'The Defiant Agents', 'The Disturbing Charm',\n",
       "       'The Enchanted April', 'The Fall Of The Grand Sarrasin',\n",
       "       'The Four Corners', 'The Garden Party, and Other Stories',\n",
       "       'The Great Gatsby', \"The King of Elfland's Daughter\",\n",
       "       'The Last Man', 'The Lighthouse', 'The Lost Kafoozalum',\n",
       "       'The Moon and Sixpence', 'The Narrative of Sojourner Truth',\n",
       "       'The Picture of Dorian Gray', 'The Planet Savers',\n",
       "       'The Scottish Chiefs', 'The Song of the Lark', 'The Story Girl',\n",
       "       'The Strange Visitation', 'The Time Traders', 'The Voyage Out',\n",
       "       'The Wonderful Wizard of Oz', 'The Youngest Girl in the School',\n",
       "       'This Side of Paradise', 'To Kill A Mockingbird',\n",
       "       'Twenty Thousand Leagues Under the Sea', \"Uncle Tom's Cabin\",\n",
       "       'Voodoo Planet'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data['y_pred']=y_test\n",
    "print(len(testing_data[testing_data['y_pred']==1].title.values))\n",
    "print(\"Book titles of other genres predicted detective:\")\n",
    "testing_data[testing_data['y_pred']==1].title.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing books predicted Horror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the n=78 testing dataset, only 10 were predicted horror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books predicted horror:\n",
      "10\n",
      "Book titles of other genres predicted horror:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Anne of Green Gables', 'Black Amazon of Mars',\n",
       "       'Chaplet Of Pearls', 'House of Mirth', 'The Deluge',\n",
       "       'The Door Through Space',\n",
       "       'The Importance of Being Earnest: A Trivial Comedy for Serious People by Oscar Wilde',\n",
       "       'The Lances of Lynwood', 'The Luckiest Girl in the School',\n",
       "       'The Mill On The Floss'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of books predicted horror:\")\n",
    "print(len(testing_data[testing_data['y_pred']==0].title.values))\n",
    "print(\"Book titles of other genres predicted horror:\")\n",
    "testing_data[testing_data['y_pred']==0].title.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ratio of detective and horror genres in the training vs. testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63.75% of the novels in the training dataset were detective, and 87.18% of the novels in the testing dataset were predicted detective. The ratio of detective to horror is much larger in the testing compared to the training, which could mean that the majority of the testing novels are closer to detective than horror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of true detective novels in the training dataset: 0.6375\n",
      "fraction of novels of testing dataset predicted detective: 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "print(\"fraction of true detective novels in the training dataset:\", len(training_data[training_data['detective']==True])/(len(training_books)))\n",
    "print(\"fraction of novels of testing dataset predicted detective:\", len(testing_data[testing_data['y_pred']==1].title.values)/len(testing_books))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is miscellaneous code used to prepare for visualizations and scatterplots that will be used in the Results and discussion section. This consists of using SVD on the entire corpus (using the vectorizer of max_features=17500) with 2 components to graph the novels in distance space in only 2 dimensions. The df books_order contains the metadata for all the books used in testing in training, adding the columns \"y\" to indicate if it was detective=1 or horror=0 (regardless if the label is true or predicted) and \"shape\" to indicate if the novel was part of the testing or training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books=training_books+testing_books\n",
    "vis=vectorizer.fit_transform(all_books)\n",
    "labels=np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: mp2\n",
    "coords_books = TruncatedSVD(n_components=2).fit_transform(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>check_1</th>\n",
       "      <th>check_2</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>author1_surname</th>\n",
       "      <th>author1_givenname</th>\n",
       "      <th>author2_surname</th>\n",
       "      <th>author2_givenname</th>\n",
       "      <th>gender_author1</th>\n",
       "      <th>...</th>\n",
       "      <th>tragedy</th>\n",
       "      <th>children</th>\n",
       "      <th>regency</th>\n",
       "      <th>manners</th>\n",
       "      <th>philosophical</th>\n",
       "      <th>coming-of-age</th>\n",
       "      <th>filename</th>\n",
       "      <th>shape</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tl566</td>\n",
       "      <td>hz542</td>\n",
       "      <td>ja532</td>\n",
       "      <td>813</td>\n",
       "      <td>1910</td>\n",
       "      <td>Leblanc</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Leblanc_813.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gc386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Strange Disappearance</td>\n",
       "      <td>1998</td>\n",
       "      <td>Green</td>\n",
       "      <td>Anna Katharine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GreenAnnaKatharine_AStrangeDisappearance.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nca28</td>\n",
       "      <td>tl566</td>\n",
       "      <td>stw43</td>\n",
       "      <td>A Study in Scarlet</td>\n",
       "      <td>1887</td>\n",
       "      <td>Conan Doyle</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ConanDoyle_AStudyInScarlet.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jc2739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agatha Webb</td>\n",
       "      <td>1899</td>\n",
       "      <td>Green</td>\n",
       "      <td>Anna Katharine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Green_AgathaWebb.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lcc82</td>\n",
       "      <td>yk499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>1872</td>\n",
       "      <td>Le_Fanu</td>\n",
       "      <td>Joseph Sheridan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Carmilla.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>jc2739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Side of Paradise</td>\n",
       "      <td>1920</td>\n",
       "      <td>Fitzgerald</td>\n",
       "      <td>F. Scott</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Fitzgerald_ThisSideOfParadise.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>vs339</td>\n",
       "      <td>thh55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To Kill A Mockingbird</td>\n",
       "      <td>1960</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Harper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Lee_ToKillAMockingbird.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>fhh26</td>\n",
       "      <td>gs542</td>\n",
       "      <td>tj256</td>\n",
       "      <td>Twenty Thousand Leagues Under the Sea</td>\n",
       "      <td>1870</td>\n",
       "      <td>Verne</td>\n",
       "      <td>Jules</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Verne_TwentyThousandLeagues.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>dgr73</td>\n",
       "      <td>jlp367</td>\n",
       "      <td>kg428</td>\n",
       "      <td>Uncle Tom's Cabin</td>\n",
       "      <td>1852</td>\n",
       "      <td>Stowe</td>\n",
       "      <td>Harriet Beecher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Stowe_UncleTom_sCabin.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cl2264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Voodoo Planet</td>\n",
       "      <td>1959</td>\n",
       "      <td>Norton</td>\n",
       "      <td>Andre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Norton_VoodooPlanet.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 check_1 check_2                                  title  year  \\\n",
       "0       tl566   hz542   ja532                                    813  1910   \n",
       "1       gc386     NaN     NaN                A Strange Disappearance  1998   \n",
       "2       nca28   tl566   stw43                     A Study in Scarlet  1887   \n",
       "3      jc2739     NaN     NaN                            Agatha Webb  1899   \n",
       "4       lcc82   yk499     NaN                               Carmilla  1872   \n",
       "..        ...     ...     ...                                    ...   ...   \n",
       "73     jc2739     NaN     NaN                  This Side of Paradise  1920   \n",
       "74      vs339   thh55     NaN                  To Kill A Mockingbird  1960   \n",
       "75      fhh26   gs542   tj256  Twenty Thousand Leagues Under the Sea  1870   \n",
       "76      dgr73  jlp367   kg428                      Uncle Tom's Cabin  1852   \n",
       "77     cl2264     NaN     NaN                          Voodoo Planet  1959   \n",
       "\n",
       "   author1_surname author1_givenname author2_surname author2_givenname  \\\n",
       "0          Leblanc           Maurice             NaN               NaN   \n",
       "1            Green    Anna Katharine             NaN               NaN   \n",
       "2      Conan Doyle            Arthur             NaN               NaN   \n",
       "3            Green    Anna Katharine             NaN               NaN   \n",
       "4          Le_Fanu   Joseph Sheridan             NaN               NaN   \n",
       "..             ...               ...             ...               ...   \n",
       "73      Fitzgerald          F. Scott             NaN               NaN   \n",
       "74             Lee            Harper             NaN               NaN   \n",
       "75           Verne             Jules             NaN               NaN   \n",
       "76           Stowe   Harriet Beecher             NaN               NaN   \n",
       "77          Norton             Andre             NaN               NaN   \n",
       "\n",
       "   gender_author1  ... tragedy children regency manners philosophical  \\\n",
       "0            male  ...   False    False   False   False         False   \n",
       "1          female  ...   False    False   False   False         False   \n",
       "2            male  ...   False    False   False   False         False   \n",
       "3          female  ...   False    False   False   False         False   \n",
       "4            male  ...   False    False   False   False         False   \n",
       "..            ...  ...     ...      ...     ...     ...           ...   \n",
       "73           male  ...   False    False   False   False         False   \n",
       "74         female  ...   False    False   False   False         False   \n",
       "75           male  ...   False    False   False   False         False   \n",
       "76         female  ...   False    False   False   False         False   \n",
       "77         female  ...   False    False   False   False         False   \n",
       "\n",
       "    coming-of-age                                      filename  shape y_pred  \\\n",
       "0           False                               Leblanc_813.txt  train    NaN   \n",
       "1           False  GreenAnnaKatharine_AStrangeDisappearance.txt  train    NaN   \n",
       "2           False                ConanDoyle_AStudyInScarlet.txt  train    NaN   \n",
       "3           False                          Green_AgathaWebb.txt  train    NaN   \n",
       "4           False                                  Carmilla.txt  train    NaN   \n",
       "..            ...                                           ...    ...    ...   \n",
       "73           True             Fitzgerald_ThisSideOfParadise.txt   test    1.0   \n",
       "74          False                    Lee_ToKillAMockingbird.txt   test    1.0   \n",
       "75          False               Verne_TwentyThousandLeagues.txt   test    1.0   \n",
       "76          False                     Stowe_UncleTom_sCabin.txt   test    1.0   \n",
       "77          False                       Norton_VoodooPlanet.txt   test    1.0   \n",
       "\n",
       "    y  \n",
       "0   1  \n",
       "1   1  \n",
       "2   1  \n",
       "3   1  \n",
       "4   0  \n",
       ".. ..  \n",
       "73  1  \n",
       "74  1  \n",
       "75  1  \n",
       "76  1  \n",
       "77  1  \n",
       "\n",
       "[158 rows x 37 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['shape']='train'\n",
    "testing_data['shape']='test'\n",
    "books_order=pd.concat([training_data, testing_data])\n",
    "books_order['y']=labels\n",
    "books_order['gender_author1']=books_order['gender_author1'].str.lower()\n",
    "books_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Results and discussion (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few paragraphs\n",
    "# realistic - 3 results\n",
    "# figure, table, accuracy table\n",
    "# analyze each of the results, one paragraph at the end how they fit together\n",
    "# group -- a bit more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reflection (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering this research question was a lot harder than we expected. We thought choosing a classification problem would mean that the process would be really similar to MP 02, but found that that was not the case. When applying what we learned from lectures, such as standardization (which was not incorporated in MP 02), we found difficulty in knowing if we were better off doing something or not. Because we are classfying a group of novels that do not have the true labels of detective and horror like the training dataset, there is no way to know if our classifer was accurate on the testing dataset. For example, our z-scores did not have a mean-0 and we could not figure out how to get the standard deviation of a sparse matrix. Additionally, figuring out the feature importance was difficult because we hadn't learned how to calculate feature importance for text data or a naive bayes classifier, but dataframes with a small defined set of variables. Additionally, we did want to try the RandomForest classifier but we did not have the computational power to fit and try several random forest classifiers. Besides techinical and coding issues, we didn't achieve many interpretable results, perhaps because of the limitations of our corpus. When answering our research question, we used a general miscellaneous set of other genres, but in fact this testing dataset is not very diverse, and contains 73% female authors. Though we were able to define a set of recommendations at the end of our research, we knew that realistically many of the pairs of novels we found were not very similar to each other by reading the synopses of these novels. If we were to change our approach and access to a more diverse corpus, we would probably get a uniform sample of \"other\" novels, or make sure the other genres were represented uniformly (e.g. romance, science fiction). We would run the experiment over other combination pairs of genres besides detective and horror, like science fiction and historical, or romance and fantasy because our research question is not restricted to only the detective and horror genre. Alternatively, we could train a classifier that can predict more than a binary category, and that we could train a model that incorporates a training dataset and testing dataset with multiple genres each (and then we could have more faith in the classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Resources consulted (0 points, but -5 if missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pre_proc()`: MP 02, INFO 3350 <br>\n",
    "`compare_scores()`: MP 02, INFO 3350<br>\n",
    "SVD Scatterplot: MP 02, INFO 3350<br>\n",
    "Lectures 12-15 INFO 3350 for reasoning in choosing a classifier and how to normalize and standardize <br>\n",
    "Feature importance for sklearn Naive Bayes: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes<br>\n",
    "MultinomialNB() sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html<br>\n",
    "LogisticRegression() sklearn documentaton: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html<br>\n",
    "TfidfVectorizer() sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Responsibility statement (0 points, but -5 if missing)\n",
    "**See separate CMS assignment 'MP 03: Responsibility statement'.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
